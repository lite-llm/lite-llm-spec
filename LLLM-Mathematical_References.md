# References

[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. *Advances in Neural Information Processing Systems (NeurIPS)*.

[2] Brown, T. B., et al. (2020). Language models are few-shot learners. *NeurIPS*.

[3] Kaplan, J., et al. (2020). Scaling laws for neural language models. *arXiv:2001.08361*.

[4] Hoffmann, J., et al. (2022). Training compute-optimal large language models. *arXiv:2203.15556*.

[5] Shazeer, N., et al. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. *ICLR*.

[6] Lepikhin, D., et al. (2020). GShard: Scaling giant models with conditional computation and automatic sharding. *arXiv:2006.16668*.

[7] Fedus, W., Zoph, B., & Shazeer, N. (2021). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. *arXiv:2101.03961*.

[8] Du, N., et al. (2022). GLaM: Efficient scaling of language models with mixture-of-experts. *ICML*.

[9] Jiang, A. Q., et al. (2024). Mixtral of experts. *arXiv:2401.04088*.

[10] Dai, D., et al. (2024). DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language models. *arXiv:2401.06066*.

[11] Cai, W., Jiang, J., Wang, F., Tang, J., Kim, S., & Huang, J. (2024). A survey on mixture of experts in large language models. *arXiv:2407.06204* (IEEE TKDE 2025).

[12] Lo, K. M., et al. (2025). A closer look into mixture-of-experts in large language models. *NAACL Findings 2025*, arXiv:2406.18219.

[13] Zhang, D., Song, J., Bi, Z., Song, X., Yuan, Y., Wang, T., Yeong, J., & Hao, J. (2025). Mixture of experts in large language models. *arXiv:2507.11181*.

[14] Rajbhandari, S., et al. (2022). DeepSpeed-MoE: Advancing mixture-of-experts inference and training to power next-generation AI scale. *arXiv:2201.05596*.

[15] Shoeybi, M., et al. (2019). Megatron-LM: Training multi-billion parameter language models using model parallelism. *arXiv:1909.08053*.

[16] Rasley, J., et al. (2020). DeepSpeed: System optimizations enable training deep learning models with over 100 billion parameters. *KDD*.

[17] Cong, P., et al. (2025). Rank also matters: Hierarchical configuration for mixture of adapter experts in LLM fine-tuning. *arXiv:2502.03884*.

[18] Liao, M., et al. (2025). HMoRA: Making LLMs more effective with hierarchical mixture of LoRA experts. *ICLR 2025*.

[19] Jordan, M. I., & Jacobs, R. A. (1994). Hierarchical mixtures of experts and the EM algorithm. *Neural Computation*.

[20] Zoph, B., et al. (2022). ST-MoE: Designing stable and transferable sparse expert models. *arXiv:2202.08906*.

[21] Wang, J., Hu, J., Cao, Q., et al. (2026). Multi-tier dynamic storage of KV cache for LLM inference under resource-constrained conditions. *Complex & Intelligent Systems*.

[22] Zou, J., et al. (2026). ContiguousKV: Accelerating LLM prefill with granularity-aligned KV cache management. *arXiv:2601.13631*.

[23] Ren, Z., et al. (2025). An I/O characterizing study of offloading LLM models and KV caches to NVMe SSD. *CHEOPS Workshop 2025*.

[24] vLLM Team (2026). Inside vLLM’s new KV offloading connector. vLLM Blog.

[25] MinIO Team (2026). Supercharging inference for AI factories: KV cache offload as a memory-hierarchy problem. MinIO Blog.

[26] Hugging Face (2023–2026). Candle: Minimalist ML framework for Rust. GitHub: https://github.com/huggingface/candle.

[27] Tracel AI (2024–2026). Burn: Next-generation tensor library and deep learning framework in Rust. https://burn.dev/.

[28] Buehler, E. L. (2025). mistral.rs: Fast, flexible LLM inference engine in pure Rust. GitHub: https://github.com/EricLBuehler/mistral.rs.

[29] Su, J., et al. (2021). RoFormer: Enhanced transformer with rotary position embedding. *arXiv:2104.09864*.

[30] Zhang, B., & Sennrich, R. (2019). Root mean square layer normalization. *arXiv:1910.07467*.

[31] Li, W., et al. (2024). Hierarchical mixture of experts: Generalizable learning for high-level synthesis. *arXiv:2410.19225*.

[32] Patro, B. N., et al. (2026). LLMOrbit: A circular taxonomy of large language models. *arXiv:2601.14053*.

[33] Pei, Z., et al. (2025). CMoE: Converting mixture-of-experts from dense to accelerate LLM inference. *arXiv:2502.04416*.

[34] Su, Z., et al. (2025). Unveiling super experts in mixture-of-experts large language models. *arXiv:2507.23279*.

[35] Sukhbaatar, S., et al. (2024). Branch-Train-MiX: Mixing expert LLMs into a mixture-of-experts LLM. *arXiv:2403.07816*.

[36] Li, Y., et al. (2024). Uni-MoE: Scaling unified multimodal LLMs with mixture of experts. *arXiv:2405.11273*.

[37] Cai, R., et al. (2024). Read-ME: Refactorizing LLMs as router-decoupled mixture of experts with system co-design. *NeurIPS 2024*.

[38] Singh, S., et al. (2023). A hybrid tensor-expert-data parallelism approach to optimize mixture-of-experts training. *ICS*.

[39] Yuan, Y., et al. (2025). X-MoE: Enabling scalable training for emerging mixture-of-experts architectures on HPC platforms. *ICS*.

[40] SuperBruceJia (2025). Awesome-Mixture-of-Experts: Curated list of MoE and MoME resources. GitHub.

[41] Friendli.ai Team (2025). The rise of MoE: Comparing 2025’s leading mixture-of-experts AI models. Friendli Blog.

[42] Swain, S. (2025). Rust: The performance edge for large language model inference. Medium.

[43] Buehler, E. L. (2025). mistral.rs 0.7.0: Now on crates.io! Reddit /r/rust.

[44] Cheng, Y., et al. (2025). An efficient KV cache layer for enterprise-scale LLM inference. LMCache Tech Report.

[45] Zhang, et al. (2025). KVSwap: Disk-aware KV cache offloading for long-context on-device inference. *arXiv:2511.11907*.

[46] Jiang, C., et al. (2025). KVPR: Efficient LLM inference with I/O-aware KV cache partial recomputation. *ACL Findings 2025*.

[47] Tang, Y., et al. (2024). Exploring CXL-based KV cache storage for LLM serving. *NeurIPS ML for Systems Workshop*.

[48] Zhong, Y., et al. (2024). Managing memory tiers with CXL in virtualized environments. *OSDI 2024*.

[49] Kim, Y. J., et al. (2022). Who says elephants can’t run: Bringing large scale MoE models into cloud scale production. *SustainNLP*.

[50] Smith, S., et al. (2022). Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B. Microsoft Research Blog.

[51] Rajbhandari, S., et al. (2022). DeepSpeed: Advancing MoE inference and training to power next-generation AI scale. Microsoft Research.

[52] Lin, Z., et al. (2025). Hierarchical mixture-of-experts model for unified code search. *Information Fusion*.

[53] Omi, et al. (2025). Similarity-preserving load-balancing for MoE routing. MoE Survey 2025.

[54] Dong, et al. (2025). MaxScore: Constrained optimization for MoE routing. MoE Survey 2025.

[55] Bai, et al. (2025). Qwen3-VL: Vision-language MoE variants.

[56] Wang, et al. (2024). HoME: Hierarchy of multi-gated experts. *arXiv:2408.05430*.

[57] Muzio, R., et al. (2024). SEER-MoE: Sparse expert routing with regularization for efficient inference. *arXiv:2404.05089*.

[58] Su, et al. (2024). CartesianMoE: Boosting knowledge sharing among experts via Cartesian product routing. *arXiv:2410.16077*.

[59] Shu, et al. (2025). MoMoE: Hierarchical mixtures at agent and neural levels. *arXiv:2511.13983*.

[60] Tracel AI (2025). Burn GitHub repository and documentation.

[61] Maio, A. (2025). Rust for AI: High-assurance infrastructure. LinkedIn.

[62] llm-d Team (2026). Native KV cache offloading to any filesystem with llm-d. llm-d Blog.

[63] Toledo, K. (2026). KV cache sharing and scale in distributed inference. llm-d Blog.

[64] NVIDIA (2026). Inference Context Memory Storage Platform (ICMSP) announcement. Blocks and Files.

[65] Patro, B. N. (2026). LLMOrbit: A circular taxonomy of large language models — from scaling walls to agentic AI systems. *arXiv:2601.14053*.

[66] del Campo, I., et al. (2026). Can LLMs do rocket science? Exploring the limits of complex reasoning with GTOC 12. *arXiv:2602.03630*.

[67] Pistilli, G., et al. (2024). CIVICS: Building a dataset for examining culturally-informed values in large language models. *arXiv:2405.13974*.

[68] Krestel, et al. (2024). Natural language processing in patents: A survey (updated 2024). *arXiv:2403.04105v2*.

[69] Guo, L., et al. (2024). PopSweeper: Automatically detecting and resolving app-blocking pop-ups. *arXiv:2412.02933*.

[70] Li, Z., et al. (2024). Your mixture-of-experts LLM is secretly an embedding model for free. *arXiv:2410.10814*.

[71] Gumaan, E., et al. (2025). ExpertRAG: Efficient RAG with mixture of experts. *arXiv:2504.08744*.

[72] Li, W., et al. (2025). Hierarchical mixture of experts: Generalizable learning for high-level synthesis. *AAAI*.

[73] Wolfe, C. R. (2025). Mixture-of-Experts (MoE): The birth and rise of conditional computation. Substack.

[74] Vijay Kumar, A. B. (2025). Mixture of experts. Medium.

[75] Emergent Mind (2026). Hierarchical mixture of experts. EmergentMind.com.

[76] MegaScale-MoE Team (2025). MegaScale-MoE: Large-scale communication-efficient training of mixture-of-experts models in production. *arXiv:2505.11432*.

[77] Yuan, Y., et al. (2025). X-MoE: Enabling scalable training for emerging mixture-of-experts architectures on HPC platforms. *ICS*.

[78] DeepSpeed Team (2025). Mixture of experts for NLG models. DeepSpeed Tutorial.

[79] DeepSpeedAI (2025). Megatron-DeepSpeed GitHub repository.

[80] Mu, B., et al. (2024). HDMoLE: Mixture of LoRA experts with hierarchical routing and dynamic thresholds for fine-tuning LLM-based ASR models. *arXiv:2409.19878*.

[81] Liao, M., et al. (2025). HiMoLE: Towards OOD-robust LoRA via hierarchical mixture of experts. *NeurIPS 2025*.

[82] Vats, A., et al. (2024). The evolution of mixture of experts: A survey from basics to breakthroughs. *Preprints.org* (updated 2025).

[83] Yang, X., et al. (2025). Mixture of experts made intrinsically interpretable. *ICML 2025*.

[84] Boix-Adsera, E., et al. (2025). Secret mixtures of experts inside your LLM. *arXiv:2512.18452*.

[85] NVIDIA Developer (2026). Optimizing communication for mixture-of-experts training with hybrid expert parallel.

[86] Samsung NAND AE Group (2026). Scaling AI inference with KV cache offloading. White Paper.

[87] Dell Technologies (2026). Scaling multi-turn LLM inference with KV cache storage offload and Dell RDMA-accelerated architecture.

[88] AWS (2025). SageMaker HyperPod now supports managed tiered KV cache and intelligent routing.

[89] Pure Storage (2026). Architecting for reuse: A deep journey into the heart of KV caching.

[90] VAST Data (2025). NVIDIA Dynamo + VAST = Scalable, optimized inference.

[91] Google Cloud (2025). Boosting LLM performance with tiered KV cache on Google Kubernetes Engine.

[92] SNIA (2025). KV-cache storage offloading for efficient inference in LLMs.

[93] NVIDIA (2026). Accelerate large-scale LLM inference and KV cache offload with CPU-GPU memory sharing.

[94] LMCache Team (2025). Efficient KV cache layer for enterprise-scale LLM inference.

[95] SGLang Team (2026). SGLang roadmap Q1 2026: Feature composability for disaggregated inference.

[96] LaurentMazare (2023–2026). tch-rs: Rust bindings for PyTorch (LibTorch). crates.io.

[97] tracel-ai (2025). Burn GitHub repository and documentation.

[98] Hugging Face (2025). Candle: Minimalist ML framework for Rust – official announcement and demos.

[99] Medium (2025). Rust for AI: High-assurance infrastructure referencing Candle, mistral.rs, Burn.

[100] Reddit /r/rust (2025). ML library comparison: Burn vs Candle.

[101] Buehler, M. J. (2025). Rust-based open source inference engine mistral.rs. LinkedIn.

[102] Athan Seal (2025). Choosing the right Rust machine learning framework: Candle, Burn, DFDX, or tch-rs. Medium.

[103] Horseee (2025). Awesome-Efficient-LLM: efficient_moe.md.

[104] MoE-Inf (2025). awesome-moe-inference: Curated collection of papers in MoE model inference.

[105] XueFuzhao (2025). awesome-mixture-of-experts.

[106] SuperBruceJia (2025). Awesome-Mixture-of-Experts.

[107] arpita8 (2025). Awesome-Mixture-of-Experts-Papers.

[108] codecaution (2025). Awesome-Mixture-of-Experts-Papers.

[109] Oliver-FutureAI (2025). Awesome-MoE.

[110] withinmiaov (2025). A-Survey-on-Mixture-of-Experts-in-LLMs.

[111] Cameron R. Wolfe (2025). Mixture-of-Experts (MoE) LLMs. Substack.

[112] Maartengrootendorst (2024). A visual guide to mixture of experts (MoE). Newsletter.

[113] IBM (2025). What is mixture of experts?

[114] Wikipedia (2026). Mixture of experts (updated with DeepSeek-V3, Qwen3-MoE).

[115] Jiang, Y., et al. (2025). Sparse models, sparse safety: Unsafe routes in mixture-of-experts LLMs. *ICLR 2026* (under review).

[116] Lu, Y., et al. (2024). Mixture-of-experts based LLM model for financial text classification. *ACM*.

[117] Sanyal, S., et al. (2025). Mixing inference-time experts for enhancing LLM reasoning. *EMNLP 2025*.

[118] NVIDIA (2026). Hybrid-EP: Optimizing communication for MoE training.

[119] Raschka, S. (2026). A dream of spring for open-weight LLMs: 10 architectures from Jan-Feb 2026. Ahead of AI.

[120] DeepSpeedAI & NVIDIA (2025). Megatron-DeepSpeed + Hybrid-EP: Production MoE training at scale. Combined technical report.
